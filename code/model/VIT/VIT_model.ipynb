{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e010e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from vit_keras import vit\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303caff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGES = \"input/\"\n",
    "# TARGET_IMAGES = \"Target/target \"\n",
    "\n",
    "TRAIN_LIMIT_SIZE = 15000\n",
    "# VALIDATION_LIMIT_SIZE = 17500\n",
    "TEST_LIMIT_SIZE = 20000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "STEPS_SIZE = TRAIN_LIMIT_SIZE // BATCH_SIZE\n",
    "\n",
    "THRESHOLD_CLEANSING = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09884a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA_RANGE = (-180,180)\n",
    "DEC_RANGE = (-90,90)\n",
    "THETA_RANGE = (0,360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5208d5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RA</th>\n",
       "      <th>DEC</th>\n",
       "      <th>THETA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.208573</td>\n",
       "      <td>0.278189</td>\n",
       "      <td>0.759450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.208573</td>\n",
       "      <td>0.278189</td>\n",
       "      <td>0.759450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.653541</td>\n",
       "      <td>0.525090</td>\n",
       "      <td>0.636799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134132</td>\n",
       "      <td>0.462668</td>\n",
       "      <td>0.167021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.680660</td>\n",
       "      <td>0.604051</td>\n",
       "      <td>0.447248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RA       DEC     THETA\n",
       "0  0.208573  0.278189  0.759450\n",
       "1  0.208573  0.278189  0.759450\n",
       "2  0.653541  0.525090  0.636799\n",
       "3  0.134132  0.462668  0.167021\n",
       "4  0.680660  0.604051  0.447248"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# importing\n",
    "df_orientation = pd.read_csv(\"orientation_data.csv\")\n",
    "df_orientation = df_orientation.drop(\"ID\",axis = 1)\n",
    "df_orientation = df_orientation.drop(\"FOV\",axis = 1)\n",
    "\n",
    "# minmax normalization\n",
    "df_orientation[\"RA\"] = (df_orientation.iloc[:,0] - RA_RANGE[0]) / (RA_RANGE[1] - RA_RANGE[0])\n",
    "df_orientation[\"DEC\"] = (df_orientation.iloc[:,1] - DEC_RANGE[0]) / (DEC_RANGE[1] - DEC_RANGE[0])\n",
    "df_orientation[\"THETA\"] = (df_orientation.iloc[:,2] - THETA_RANGE[0]) / (THETA_RANGE[1] - THETA_RANGE[0])\n",
    "df_orientation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39420318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_shape = (512, 512,3)\n",
    "\n",
    "def load_images(image_name):\n",
    "    img = cv2.imread(image_name)\n",
    "    img = transform.resize(img,image_shape)\n",
    "    return img\n",
    "\n",
    "def image_generator():\n",
    "    while True:\n",
    "        for i in range(0,STEPS_SIZE):\n",
    "            batch_x_train = []\n",
    "            for j in range(i*BATCH_SIZE,(i+1)*BATCH_SIZE):\n",
    "                batch_x_train.append(load_images(INPUT_IMAGES+str(j+1)+\".png\"))\n",
    "            yield np.array(batch_x_train), df_orientation.iloc[(i*BATCH_SIZE):((i+1) * BATCH_SIZE),:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf093e",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da625c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ffce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48221e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = vit.vit_l16(image_size = (512,512), activation = \"sigmoid\", include_top = False, \n",
    "                         pretrained=True, pretrained_top=False, weights='imagenet21k+imagenet2012' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac3ab30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "beeaf8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6415ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layers\n",
    "x = base_model.output\n",
    "x = Dense(256, activation='relu')(x)\n",
    "output_layer = Dense(3, activation='sigmoid')(x)  # 3 sigmoid nodes for RA, DEC, and THETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd3e5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs = output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f225ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=\"adam\", loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02f98d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 512, 512, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 32, 32, 1024)      787456    \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 1024, 1024)        0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 1025, 1024)        1024      \n",
      "                                                                 \n",
      " Transformer/posembed_input  (None, 1025, 1024)        1049600   \n",
      "  (AddPositionEmbs)                                              \n",
      "                                                                 \n",
      " Transformer/encoderblock_0  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_3  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_4  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_5  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_6  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_7  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_8  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_9  ((None, 1025, 1024),      12596224  \n",
      "  (TransformerBlock)          (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 0 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 1 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 2 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 3 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 4 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 5 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 6 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 7 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 8 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 1025, 1024),      12596224  \n",
      " 9 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 1025, 1024),      12596224  \n",
      " 0 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 1025, 1024),      12596224  \n",
      " 1 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 1025, 1024),      12596224  \n",
      " 2 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 1025, 1024),      12596224  \n",
      " 3 (TransformerBlock)         (None, 16, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoder_norm (  (None, 1025, 1024)        2048      \n",
      " LayerNormalization)                                             \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 304412675 (1.13 GB)\n",
      "Trainable params: 263171 (1.00 MB)\n",
      "Non-trainable params: 304149504 (1.13 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7af79",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c66ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"vit_model\", monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8a151c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = image_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcc669b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 0.0579  INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 36285s 78s/step - loss: 0.0579\n",
      "Epoch 2/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 0.0499  INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 36804s 79s/step - loss: 0.0499\n",
      "Epoch 3/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 0.0480  INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 38424s 82s/step - loss: 0.0480\n",
      "Epoch 4/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 0.0469  INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 47563s 102s/step - loss: 0.0469\n",
      "Epoch 5/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 0.0460  INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 47009s 100s/step - loss: 0.0460\n",
      "Epoch 6/100\n",
      "376/468 [=======================>......] - ETA: 2:46:27 - loss: 0.0455"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/Transformer/encoderblock_2/MultiHeadDotProductAttention_1/MatMul' defined at (most recent call last):\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Farid.Guliyev\\AppData\\Local\\Temp\\ipykernel_15068\\3396471353.py\", line 1, in <module>\n      hist = model.fit(\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\vit_keras\\layers.py\", line 169, in call\n      x, weights = self.att(x)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\vit_keras\\layers.py\", line 107, in call\n      attention, weights = self.attention(query, key, value)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\vit_keras\\layers.py\", line 87, in attention\n      score = tf.matmul(query, key, transpose_b=True)\nNode: 'model_1/Transformer/encoderblock_2/MultiHeadDotProductAttention_1/MatMul'\nOOM when allocating tensor with shape[32,16,1025,1025] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node model_1/Transformer/encoderblock_2/MultiHeadDotProductAttention_1/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_160338]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15068\\3396471353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hist = model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSTEPS_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/Transformer/encoderblock_2/MultiHeadDotProductAttention_1/MatMul' defined at (most recent call last):\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Farid.Guliyev\\AppData\\Local\\Temp\\ipykernel_15068\\3396471353.py\", line 1, in <module>\n      hist = model.fit(\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\vit_keras\\layers.py\", line 169, in call\n      x, weights = self.att(x)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\vit_keras\\layers.py\", line 107, in call\n      attention, weights = self.attention(query, key, value)\n    File \"C:\\Users\\Farid.Guliyev\\anaconda3\\lib\\site-packages\\vit_keras\\layers.py\", line 87, in attention\n      score = tf.matmul(query, key, transpose_b=True)\nNode: 'model_1/Transformer/encoderblock_2/MultiHeadDotProductAttention_1/MatMul'\nOOM when allocating tensor with shape[32,16,1025,1025] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node model_1/Transformer/encoderblock_2/MultiHeadDotProductAttention_1/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_160338]"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = STEPS_SIZE,\n",
    "    epochs = 100,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
